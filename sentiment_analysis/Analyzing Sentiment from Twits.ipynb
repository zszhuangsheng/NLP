{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sentiment': 2, 'message_body': '$FITB great buy at 26.00...ill wait', 'timestamp': '2018-07-01T00:00:09Z'}, {'sentiment': 1, 'message_body': '@StockTwits $MSFT', 'timestamp': '2018-07-01T00:00:42Z'}, {'sentiment': 2, 'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'timestamp': '2018-07-01T00:01:24Z'}, {'sentiment': 1, 'message_body': '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'timestamp': '2018-07-01T00:01:47Z'}, {'sentiment': 0, 'message_body': '$AMD reveal yourself!', 'timestamp': '2018-07-01T00:02:13Z'}, {'sentiment': 1, 'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'timestamp': '2018-07-01T00:03:10Z'}, {'sentiment': -2, 'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'timestamp': '2018-07-01T00:04:09Z'}, {'sentiment': 1, 'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'timestamp': '2018-07-01T00:04:17Z'}, {'sentiment': 2, 'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'timestamp': '2018-07-01T00:08:01Z'}, {'sentiment': -2, 'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'timestamp': '2018-07-01T00:09:03Z'}]\n"
     ]
    }
   ],
   "source": [
    "with open('twits.json') as f:\n",
    "    twits = json.load(f)\n",
    "    \n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "sentiments = [twit['sentiment']+2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$FITB great buy at 26.00...ill wait', '@StockTwits $MSFT', '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', '$AMD reveal yourself!', '$AAPL Why the drop? I warren Buffet taking out his position?', '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', '$BAC ok good we&#39;re not dropping in price over the weekend, lol', '$AMAT - Daily Chart, we need to get back to above 50.', '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?']\n",
      "[4, 3, 4, 3, 2, 3, 0, 3, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "print(messages[:10])\n",
    "print(sentiments[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhuangsheng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove useless info\n",
    "# convert the message into token\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    text = message.lower()\n",
    "    text = re.sub(r\"http?://[^\\s]+\",\" \", text)\n",
    "    text = re.sub(r\"\\$[a-zA-Z0-9]*\",\" \", text)\n",
    "    text = re.sub(r\"@[a-zA-Z0-9]*\",\" \", text)\n",
    "    text = re.sub(r\"[^a-z]\",\" \", text)\n",
    "    tokens = text.split()\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w) for w in tokens if len(w) > 1]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'buy', 'at', 'ill', 'wait'], [], ['staanalystalert', 'for', 'jefferies', 'maintains', 'with', 'rating', 'of', 'hold', 'setting', 'target', 'price', 'at', 'usd', 'our', 'own', 'verdict', 'is', 'buy']]\n"
     ]
    }
   ],
   "source": [
    "tokenized = [preprocess(message) for message in messages]\n",
    "print(tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before removing empty ones:1548010\n",
      "Number of tokens after removing empty ones:1510516\n"
     ]
    }
   ],
   "source": [
    "# remove empty ones\n",
    "\n",
    "print(\"Number of tokens before removing empty ones:{}\".format(len(tokenized)))\n",
    "good_tokens_idx = [idx for idx, token in enumerate(tokenized) if len(token) > 0]\n",
    "tokenized = [tokenized[idx] for idx in good_tokens_idx]\n",
    "sentiments = [sentiments[idx] for idx in good_tokens_idx]\n",
    "\n",
    "print(\"Number of tokens after removing empty ones:{}\".format(len(tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'is', 'for', 'on', 'http', 'of', 'and', 'in', 'this', 'com', 'it', 'at', 'will', 'amp']\n",
      "7034\n"
     ]
    }
   ],
   "source": [
    "# Bag of words\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "## flatten the nest list\n",
    "stacked_tokens = [word for twit in tokenized for word in twit]\n",
    "## genenrate bow\n",
    "bow = Counter(stacked_tokens)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "### Dictionart that contains the Frequency of words appearing in messages.\n",
    "### The key is the token and the value is the frequency of that word in the corpus.\n",
    "\n",
    "total_num_words = len(stacked_tokens)\n",
    "freqs = {key:value/total_num_words for key, value in bow.items()}\n",
    "\n",
    "### Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 5e-6\n",
    "\n",
    "### Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "high_cutoff = 15\n",
    "\n",
    "### The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "K_most_common = [word[0] for word in bow.most_common(high_cutoff)]\n",
    "\n",
    "### remove high qand low freq words\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "print(K_most_common)\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprare model used Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510516/1510516 [21:37<00:00, 1164.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "vocab = {word:ii+1 for ii, word in enumerate(filtered_words)}\n",
    "id2vocab = {ii:word for ii, word in enumerate(filtered_words)}\n",
    "\n",
    "filtered = []\n",
    "for twit in tqdm(tokenized):\n",
    "    filtered.append([word for word in twit if word in filtered_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19937528069962393"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## balancing the classes\n",
    "\n",
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment)\n",
    "        \n",
    "## check the result\n",
    "\n",
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert tokens into integer ids used in network\n",
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the training data ans test data\n",
    "## Data Loader\n",
    "\n",
    "def dataloader(messages, labels, sequence_length, batch_size, shuffle=False):\n",
    "    # shulffe the data\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        sentiments = [messages[idx] for idx in indices]\n",
    "    # get total numbers of messages\n",
    "    total_sequences = len(messages)\n",
    "    \n",
    "    for ii in range(0,total_sequences, batch_size):\n",
    "        batch_messages = messages[ii:ii+batch_size] # output:[[idx1,idx2,idx3],[idx4],[idx5,idx6],...]\n",
    "        # all 0 \n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # LEFT PAD!\n",
    "            start_idx = max(sequence_length - len(token_tensor),0)\n",
    "            batch[start_idx:,batch_num] = token_tensor[:sequence_length] # replace with column\n",
    "        label_tensor = torch.tensor(labels[ii:ii+len(batch_messages)])\n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training data and validation set\n",
    "valid_split = int(0.9 * len(token_ids))\n",
    "\n",
    "train_features = token_ids[:valid_split]\n",
    "valid_features = token_ids[valid_split:]\n",
    "train_labels = sentiments[:valid_split]\n",
    "valid_labels = sentiments[valid_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [2365, 3082,  488,  ..., 6160, 6509, 1555],\n",
      "        [2558, 5952, 3137,  ..., 3232, 5848, 5324],\n",
      "        [5767, 2365,  748,  ..., 3005, 4889, 5777]])\n",
      "tensor([4, 4, 3, 3, 0, 3, 4, 0, 4, 0, 4, 4, 4, 3, 4, 4, 3, 3, 3, 0, 1, 3, 3, 0,\n",
      "        4, 4, 0, 3, 2, 1, 3, 3, 4, 3, 4, 3, 3, 2, 3, 3, 3, 3, 1, 4, 3, 4, 0, 3,\n",
      "        2, 0, 1, 4, 1, 3, 2, 4, 1, 0, 4, 4, 2, 4, 2, 3, 4, 0, 2, 0, 0, 3, 1, 1,\n",
      "        2, 3, 0, 2, 0, 4, 4, 2, 2, 3, 3, 4, 0, 4, 3, 3, 2, 4, 2, 4, 1, 1, 4, 3,\n",
      "        2, 2, 4, 1, 2, 0, 3, 0, 4, 2, 4, 3, 2, 3, 1, 4, 1, 2, 2, 3, 2, 0, 0, 2,\n",
      "        2, 0, 1, 3, 2, 3, 0, 4, 2, 4, 4, 1, 0, 2, 1, 3, 4, 1, 4, 3, 0, 3, 1, 2,\n",
      "        3, 2, 2, 3, 2, 2, 1, 4, 1, 1, 3, 0, 3, 3, 1, 4, 3, 0, 3, 1, 3, 3, 3, 4,\n",
      "        2, 2, 0, 1, 2, 0, 0, 0, 0, 3, 0, 1, 3, 4, 3, 2, 3, 1, 1, 4, 2, 3, 3, 3,\n",
      "        1, 1, 2, 1, 0, 3, 4, 2, 4, 2, 0, 3, 2, 0, 4, 4, 3, 2, 3, 2, 3, 0, 0, 2,\n",
      "        0, 1, 2, 3, 1, 1, 3, 3, 1, 2, 3, 0, 1, 4, 3, 0, 0, 2, 2, 3, 3, 2, 2, 4,\n",
      "        3, 1, 3, 4, 0, 2, 3, 4, 4, 3, 2, 4, 3, 3, 1, 2, 1, 4, 3, 3, 3, 4, 2, 3,\n",
      "        0, 2, 3, 0, 1, 3, 2, 1, 2, 1, 3, 0, 4, 2, 0, 4, 3, 0, 3, 0, 0, 1, 2, 0,\n",
      "        0, 4, 0, 0, 1, 0, 3, 1, 0, 3, 3, 4, 3, 1, 3, 0, 0, 2, 1, 2, 3, 0, 3, 0,\n",
      "        1, 2, 1, 2, 0, 4, 4, 2, 0, 1, 4, 2, 0, 3, 2, 4, 0, 2, 0, 2, 2, 4, 0, 3,\n",
      "        4, 1, 2, 1, 1, 2, 1, 1, 3, 0, 3, 1, 0, 0, 3, 4, 1, 3, 0, 2, 4, 0, 0, 1,\n",
      "        4, 0, 1, 0, 2, 1, 3, 4, 4, 0, 3, 0, 0, 3, 1, 0, 0, 1, 1, 1, 0, 3, 2, 0,\n",
      "        3, 4, 0, 1, 2, 2, 4, 4, 1, 0, 0, 3, 2, 0, 3, 0, 2, 1, 1, 2, 1, 0, 0, 1,\n",
      "        2, 1, 1, 3, 0, 4, 3, 0, 1, 4, 1, 3, 0, 3, 4, 4, 4, 4, 4, 1, 0, 4, 4, 2,\n",
      "        4, 0, 4, 0, 0, 3, 0, 2, 0, 3, 1, 3, 4, 3, 1, 3, 3, 3, 3, 0, 3, 2, 3, 3,\n",
      "        0, 2, 3, 1, 4, 0, 0, 1, 3, 0, 1, 3, 2, 3, 2, 4, 4, 0, 4, 4, 2, 3, 0, 3,\n",
      "        0, 0, 0, 3, 2, 4, 4, 4, 0, 0, 3, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 4, 4,\n",
      "        3, 0, 1, 0, 3, 4, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "## view text_batch and labels\n",
    "text_batch, labels = next(iter(dataloader(train_features, \n",
    "                                          train_labels, \n",
    "                                          sequence_length=40, \n",
    "                                          batch_size = 512)))\n",
    "print(text_batch)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, \n",
    "                 lstm_size, output_size, lstm_layers = 1, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings = self.vocab_size,\n",
    "                                     embedding_dim = self.embed_size\n",
    "                                     )\n",
    "        # LSTM \n",
    "        self.lstm = nn.LSTM(input_size = self.embed_size,\n",
    "                            hidden_size = self.lstm_size,\n",
    "                            num_layers = self.lstm_layers,\n",
    "                            batch_first = False,\n",
    "                            dropout = self.dropout\n",
    "                           )\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(in_features = self.lstm_size,\n",
    "                            out_features = self.output_size)\n",
    "        # \n",
    "        self.log_smax = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        #train_on_gpu = False\n",
    "        \n",
    "        if False: #torch.cuda.isavailable():\n",
    "            hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_().cuda(),\n",
    "                      weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.lstm_layers,batch_size, self.lstm_size).zero_(),\n",
    "                      weight.new(self.lstm_layers,batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "        \"\"\"\n",
    "        embed = self.embedding(nn_input)\n",
    "        lstm_out, hidden_state = self.lstm(embed, hidden_state)\n",
    "        lstm_out = lstm_out[-1]\n",
    "        \n",
    "        logps = self.log_smax(self.dropout(self.fc(lstm_out)))\n",
    "        return logps, hidden_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4411, -1.7025, -2.1722, -2.0416, -1.0866],\n",
      "        [-1.7919, -1.5801, -2.0999, -1.9775, -1.0037],\n",
      "        [-1.9319, -1.6593, -1.4829, -2.1431, -1.1376],\n",
      "        [-1.8996, -1.6231, -2.2218, -1.4598, -1.1635]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Model View Test\n",
    "model = TextClassifier(len(vocab), 10,6,5, dropout = 0.1, lstm_layers = 2)\n",
    "model.embedding.weight.data.uniform_(-1,1)\n",
    "input = torch.randint(0,1000,(5,4), dtype = torch.int64)\n",
    "\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps,_ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(7035, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (log_smax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers = 2, dropout = 0.2)\n",
    "model.embedding.weight.data.uniform_(-1,1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch:1/3... Step:100... Loss:1.570675... Val Loss:1.594005... Val Accuracy:0.272268...\n",
      "New best accuracy - model saved\n",
      "Epoch:1/3... Step:200... Loss:1.593549... Val Loss:1.592026... Val Accuracy:0.272403...\n",
      "New best accuracy - model saved\n",
      "Epoch:1/3... Step:300... Loss:1.573412... Val Loss:1.594591... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:400... Loss:1.553703... Val Loss:1.592939... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:500... Loss:1.572526... Val Loss:1.592968... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:600... Loss:1.582663... Val Loss:1.597873... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:700... Loss:1.593295... Val Loss:1.593351... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:800... Loss:1.563199... Val Loss:1.594091... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:900... Loss:1.516518... Val Loss:1.599866... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1000... Loss:1.570485... Val Loss:1.594632... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1100... Loss:1.589500... Val Loss:1.591533... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1200... Loss:1.569356... Val Loss:1.595700... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1300... Loss:1.572914... Val Loss:1.593107... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1400... Loss:1.558619... Val Loss:1.593893... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1500... Loss:1.595500... Val Loss:1.591938... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1600... Loss:1.616509... Val Loss:1.591877... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1700... Loss:1.624723... Val Loss:1.591720... Val Accuracy:0.272403...\n",
      "Epoch:1/3... Step:1800... Loss:1.604282... Val Loss:1.591875... Val Accuracy:0.272403...\n",
      "Starting epoch 2\n",
      "Epoch:2/3... Step:100... Loss:1.574565... Val Loss:1.596260... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:200... Loss:1.581898... Val Loss:1.591950... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:300... Loss:1.591200... Val Loss:1.594231... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:400... Loss:1.549652... Val Loss:1.592687... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:500... Loss:1.566955... Val Loss:1.592808... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:600... Loss:1.578557... Val Loss:1.597446... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:700... Loss:1.603046... Val Loss:1.593162... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:800... Loss:1.555438... Val Loss:1.593967... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:900... Loss:1.522087... Val Loss:1.599193... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1000... Loss:1.571376... Val Loss:1.594264... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1100... Loss:1.587237... Val Loss:1.591945... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1200... Loss:1.575860... Val Loss:1.595264... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1300... Loss:1.582013... Val Loss:1.593085... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1400... Loss:1.548553... Val Loss:1.594344... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1500... Loss:1.592965... Val Loss:1.592117... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1600... Loss:1.614560... Val Loss:1.592201... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1700... Loss:1.631222... Val Loss:1.592118... Val Accuracy:0.272403...\n",
      "Epoch:2/3... Step:1800... Loss:1.596356... Val Loss:1.591897... Val Accuracy:0.272403...\n",
      "Starting epoch 3\n",
      "Epoch:3/3... Step:100... Loss:1.563612... Val Loss:1.596012... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:200... Loss:1.582187... Val Loss:1.592014... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:300... Loss:1.579700... Val Loss:1.594677... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:400... Loss:1.545167... Val Loss:1.592982... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:500... Loss:1.562891... Val Loss:1.592820... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:600... Loss:1.594267... Val Loss:1.597249... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:700... Loss:1.614370... Val Loss:1.593227... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:800... Loss:1.551692... Val Loss:1.593911... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:900... Loss:1.519195... Val Loss:1.600103... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1000... Loss:1.567890... Val Loss:1.594140... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1100... Loss:1.591860... Val Loss:1.591564... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1200... Loss:1.572365... Val Loss:1.595392... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1300... Loss:1.572751... Val Loss:1.593125... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1400... Loss:1.553358... Val Loss:1.594538... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1500... Loss:1.596778... Val Loss:1.592152... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1600... Loss:1.625060... Val Loss:1.592107... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1700... Loss:1.626955... Val Loss:1.591960... Val Accuracy:0.272403...\n",
      "Epoch:3/3... Step:1800... Loss:1.600729... Val Loss:1.591847... Val Accuracy:0.272403...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 512\n",
    "sequence_length = 40\n",
    "learning_rate = 0.003\n",
    "clip = 5\n",
    "best_val_acc = 0\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=sequence_length, shuffle=True):\n",
    "        if text_batch.size()!=torch.Size([sequence_length,batch_size]):\n",
    "            continue\n",
    "        steps += 1\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model----------------------------------------------------------------*\n",
    "        model.zero_grad()\n",
    "        log_ps, hidden = model.forward(text_batch, hidden)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            val_accuracy = []\n",
    "            val_hidden = model.init_hidden(batch_size)\n",
    "            \n",
    "            for val_text_batch, val_labels in dataloader(\n",
    "            valid_features, valid_labels, batch_size = batch_size, sequence_length = sequence_length):\n",
    "                if val_text_batch.size() != torch.Size([sequence_length, batch_size]):\n",
    "                    continue\n",
    "                val_text_batch, val_labels = val_text_batch.to(device), val_labels.to(device)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                for each in val_hidden:\n",
    "                    each.to(device)\n",
    "                val_log_ps, hidden = model.forward(val_text_batch, val_hidden)\n",
    "                \n",
    "                val_loss = criterion(val_log_ps.squeeze(), val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                val_ps = torch.exp(val_log_ps)\n",
    "                top_p, top_class = val_ps.topk(1, dim=1)\n",
    "                equals = top_class == val_labels.view(*top_class.shape)\n",
    "                val_accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n",
    "                \n",
    "            # TODO Implement: Print metrics---------------------------------------------------------------*\n",
    "            model.train()\n",
    "            this_val_acc = sum(val_accuracy)/len(val_accuracy)\n",
    "            \n",
    "            print(\"Epoch:{}/{}...\".format(epoch+1, epochs),\n",
    "                  \"Step:{}...\".format(steps),\n",
    "                  \"Loss:{:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss:{:.6f}...\".format(sum(val_losses)/len(val_losses)),\n",
    "                  \"Val Accuracy:{:.6f}...\".format(this_val_acc))\n",
    "            if this_val_acc > best_val_acc:\n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'step':steps,\n",
    "                #'model_state_dict': model.state_dic(),\n",
    "                #'optimizer_state_dict': optimizer.stat_dic(),\n",
    "                'loss': loss,\n",
    "                }, 'best_model')\n",
    "                best_val_acc = this_val_acc\n",
    "                print(\"New best accuracy - model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuangsheng/opt/anaconda3/envs/env_zipline/lib/python3.5/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TextClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    # TODO Implement\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    # Filter non-vocab words\n",
    "    tokens = [word for word in tokens if word in filtered_words]\n",
    "    \n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[word] for word in tokens]\n",
    "        \n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.tensor(tokens).unsqueeze(1)\n",
    "\n",
    "    # Get the NN output\n",
    "    hidden = model.init_hidden(text_input.size(1))\n",
    "    logps, _ = model.forward(text_input,hidden)\n",
    "    \n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.exp(logps)\n",
    "    \n",
    "    return pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1770924 , 0.18436691, 0.20150499, 0.24226455, 0.19477111]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use one message as test\n",
    "\n",
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_twits.json') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Twit Stream\n",
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall(r\"\\$[A-Z]{2,4}\", text)\n",
    "        score = predict(text, model, vocab)\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': array([[0.17843668, 0.18518604, 0.20136043, 0.24037914, 0.19463772]],\n",
       "       dtype=float32), 'symbol': '$AAPL', 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
